#!/bin/bash
set -e

if [ "$1" = "" ] || [ "$1" = "-h" ] || [ "$1" = "--help" ]; then
    echo "Usage: $0 SECTION/USER [YOUR_USERNAME]
Downloads the entire gallery/scraps/favorites of any furaffinity.net user.

Examples:
 $0 gallery/kodardragon
 $0 scraps/---
 $0 favorites/kivuli

You can also log in to FurAffinity and download restricted content, like this:
 $0 gallery/gonnaneedabiggerboat /path/to/your/cookies.txt"
    exit 1
fi

runtime_dir="$HOME"'/.cache/furaffinity-dl'
mkdir -p "$runtime_dir"
tempfile1="$(umask u=rwx,g=,o= && mktemp $runtime_dir/fa-dl.XXXXXXXXXX)"
tempfile2="$(umask u=rwx,g=,o= && mktemp $runtime_dir/fa-dl.XXXXXXXXXX)"

cleanup() {
    rm -r "$tempfile1"
	rm -r "$tempfile2"
}
trap cleanup EXIT

COOKIES_FILE="$2"
if [ "$COOKIES_FILE" = "" ]; then
    # set a wget wrapper with custom user agent
    fwget() {
        wget --user-agent="Mozilla/5.0 furaffinity-dl (https://github.com/Shnatsel/furaffinity-dl)" $*
    }
else
    # set a wget wrapper with custom user agent and cookies
    fwget() {
        wget --user-agent="Mozilla/5.0 furaffinity-dl (https://github.com/Shnatsel/furaffinity-dl)" \
        --load-cookies "$COOKIES_FILE" $*
    }
fi

echo "All Page Urls" > all_urls.txt

base_url=https://www.furaffinity.net/$1

url="$base_url"

# Iterate over the gallery pages with thumbnails and links to artwork view pages
while true; do
    fwget -O "$tempfile1" "$url"
    if [ "$COOKIES_FILE" != "" ] && grep -q 'furaffinity.net/login/' "$tempfile1"; then
        echo "--------------
	
ERROR: You have provided a cookies file, but it does not contain valid cookies.

If this file used to work, this means that the cookies have expired;
you will have to log in to FurAffinity from your web browser and export the cookies again.

If this is the first time you're trying to use cookies, make sure you have exported them
in Netscape format (this is normally done through \"cookie export\" browser extensions)
and supplied the correct path to the cookies.txt file to this script.

If that doesn't resolve the issue, please report the problem at
https://github.com/Shnatsel/furaffinity-dl/issues" >&2
        exit 1
    fi
    # check if we've reached last page
    grep -q -i "there are no submissions to list" "$tempfile" && break
	# Extract links to pages with individual artworks and iterate over them
    artwork_pages=$(grep '<a href="/view/' "$tempfile1" | grep -E --only-matching '/view/[[:digit:]]+/' | uniq)

	for page in $artwork_pages; do
        # Download the submission page
        fwget -O "$tempfile2" 'https://www.furaffinity.net'"$page"

        if grep -q "System Message" "$tempfile"; then
            echo "WARNING: $page seems to be inaccessible, skipping."
            continue
        fi
	
		#pulls artist name from page, removing special characters
		artist_name=$(grep -oP -m 1 '(?<=<img class="avatar" alt=").*(?=" src)' "$tempfile2" | tr -dc '[:alnum:]')
        
		# Get the full size image URL.
        # This will be a facdn.net link, we have to use HTTP
        # to get around DPI-based page blocking in some countries.
        art_url='http:'$(grep -E --max-count=1 --only-matching 'href="//d\.facdn\.net/art/.+">Download' "$tempfile2" | cut -d '"' -f 2)
		# If no image was found, it could be music, try for music
		if [ -z art_url ]; then
			art_url='http:'$(grep -E --max-count=1 --only-matching 'href="//d\.facdn\.net/art/music/.+">Download' "$tempfile2" | cut -d '"' -f 2)
		fi
		#If music was not found, try for story
		if [ -z art_url ]; then
			art_url='http:'$(grep -E --max-count=1 --only-matching 'href="//d\.facdn\.net/art/stories/.+">Download' "$tempfile2" | cut -d '"' -f 2)
		fi
				
		if [ ! -d $artist_name ]; then 
			mkdir $artist_name
		fi
        # TODO: Get the submission title out of the page
        # this trick may come in handy for avoiding slashes in filenames:
        # | tr '/' 'âˆ•'
		cd "$artist_name"
        wget -N --timestamping "$art_url" || echo "Failed: $art_url" >> failed_downloads.txt
		cd ".."
    done

	#Gets the next page url, this is no longer standard for favs, so incrementing page count no longer works. 
	next_page_url=$(grep -E "href=\"/$1/[[:digit:]]+/[[:alpha:]]{0,4}\">Next" "$tempfile1"  | grep -E -m 1 --only-matching "$1/[[:digit:]]+/[[:alpha:]]{0,4}") ||  next_page_url=""
	if [ -z $next_page_url ]; then
		echo "Did not find Next page, download complete!"
		exit 0
	fi
	
	url="https://www.furaffinity.net/"$next_page_url
	echo "-------------------------------------------------"
	echo "Next page Url" $next_page_url 
	echo $url >> all_urls.txt
	echo "-------------------------------------------------"
done
